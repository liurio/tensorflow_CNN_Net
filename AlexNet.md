# AlexNet

## 1 背景

AlexNet是2012年ImageNet竞赛冠军获得者Hinton和他的学生Alex Krizhevsky设计的。也是在那年之后，更多的更深的神经网路被提出，比如优秀的vgg,GoogleLeNet。其官方提供的数据模型，准确率达到57.1%,top 1-5 达到80.2%. 这项对于传统的机器学习分类算法而言，已经相当的出色。 

![image](https://raw.githubusercontent.com/liurio/deep_learning/master/img/AlexNetGPU.png)

## 2 网络结构

上边是两台GPU服务器，所以会看到两个流程。AlexNet网络结构如下图所示：

![image](https://raw.githubusercontent.com/liurio/deep_learning/master/img/AlexNet网络结构.png)

## 3 各层详解

### 3.1 卷积层1

第一层输入数据为原始的2272273的图像，这个图像被11113的卷积核进行卷积运算，卷积核对原始图像的每次卷积都生成一个新的像素。卷积核沿原始图像的x轴方向和y轴方向两个方向移动，移动的步长是4个像素。因此，卷积核在移动的过程中会生成(227-11)/4+1=55个像素(227个像素减去11，正好是54，即生成54个像素，再加上被减去的11也对应生成一个像素)，行和列的5555个像素形成对原始图像卷积之后的像素层。共有96个卷积核，会生成555596个卷积后的像素层。96个卷积核分成2组，每组48个卷积核。对应生成2组555548的卷积后的像素层数据。这些像素层经过relu1单元的处理，生成激活像素层，尺寸仍为2组5555*48的像素层数据。

这些像素层经过pool运算(池化运算)的处理，池化运算的尺度为33，运算的步长为2，则池化后图像的尺寸为(55-3)/2+1=27。 即池化后像素的规模为272796；然后经过归一化处理，归一化运算的尺度为55；第一卷积层运算结束后形成的像素层的规模为272796。分别对应96个卷积核所运算形成。这96层像素层分为2组，每组48个像素层，每组在一个独立的GPU上进行运算。

反向传播时，每个卷积核对应一个偏差值。即第一层的96个卷积核对应上层输入的96个偏差值。

### 3.2 卷积层2

第二层输入数据为第一层输出的272796的像素层，为便于后续处理，每幅像素层的左右两边和上下两边都要填充2个像素；272796的像素数据分成272748的两组像素数据，两组数据分别再两个不同的GPU中进行运算。每组像素数据被5548的卷积核进行卷积运算，卷积核对每组数据的每次卷积都生成一个新的像素。卷积核沿原始图像的x轴方向和y轴方向两个方向移动，移动的步长是1个像素。因此，卷积核在移动的过程中会生成(27-5+22)/1+1=27个像素。(27个像素减去5，正好是22，在加上上下、左右各填充的2个像素，即生成26个像素，再加上被减去的5也对应生成一个像素)，行和列的2727个像素形成对原始图像卷积之后的像素层。共有256个5548卷积核；这256个卷积核分成两组，每组针对一个GPU中的272748的像素进行卷积运算。会生成两组2727128个卷积后的像素层。这些像素层经过relu2单元的处理，生成激活像素层，尺寸仍为两组2727128的像素层。

这些像素层经过pool运算(池化运算)的处理，池化运算的尺度为33，运算的步长为2，则池化后图像的尺寸为(57-3)/2+1=13。 即池化后像素的规模为2组1313128的像素层；然后经过归一化处理，归一化运算的尺度为55；第二卷积层运算结束后形成的像素层的规模为2组1313128的像素层。分别对应2组128个卷积核所运算形成。每组在一个GPU上进行运算。即共256个卷积核，共2个GPU进行运算。

反向传播时，每个卷积核对应一个偏差值。即第一层的96个卷积核对应上层输入的256个偏差值。

### 3.3 卷积层3

第三层输入数据为第二层输出的2组1313128的像素层；为便于后续处理，每幅像素层的左右两边和上下两边都要填充1个像素；2组像素层数据都被送至2个不同的GPU中进行运算。每个GPU中都有192个卷积核，每个卷积核的尺寸是33256。因此，每个GPU中的卷积核都能对2组1313128的像素层的所有数据进行卷积运算。卷积核对每组数据的每次卷积都生成一个新的像素。卷积核沿像素层数据的x轴方向和y轴方向两个方向移动，移动的步长是1个像素。因此，运算后的卷积核的尺寸为(13-3+12)/1+1=13（13个像素减去3，正好是10，在加上上下、左右各填充的1个像素，即生成12个像素，再加上被减去的3也对应生成一个像素），每个GPU中共1313192个卷积核。2个GPU中共1313384个卷积后的像素层。这些像素层经过relu3单元的处理，生成激活像素层，尺寸仍为2组1313192像素层，共1313*384个像素层。 

### 3.4 卷积层4

第四层输入数据为第三层输出的2组1313192的像素层；为便于后续处理，每幅像素层的左右两边和上下两边都要填充1个像素；2组像素层数据都被送至2个不同的GPU中进行运算。每个GPU中都有192个卷积核，每个卷积核的尺寸是33192。因此，每个GPU中的卷积核能对1组1313192的像素层的数据进行卷积运算。卷积核对每组数据的每次卷积都生成一个新的像素。卷积核沿像素层数据的x轴方向和y轴方向两个方向移动，移动的步长是1个像素。因此，运算后的卷积核的尺寸为(13-3+12)/1+1=13（13个像素减去3，正好是10，在加上上下、左右各填充的1个像素，即生成12个像素，再加上被减去的3也对应生成一个像素），每个GPU中共1313192个卷积核。2个GPU中共1313384个卷积后的像素层。这些像素层经过relu4单元的处理，生成激活像素层，尺寸仍为2组1313192像素层，共1313*384个像素层。 

### 3.5 卷积层5

第五层输入数据为第四层输出的2组1313192的像素层；为便于后续处理，每幅像素层的左右两边和上下两边都要填充1个像素；2组像素层数据都被送至2个不同的GPU中进行运算。每个GPU中都有128个卷积核，每个卷积核的尺寸是33192。因此，每个GPU中的卷积核能对1组1313192的像素层的数据进行卷积运算。卷积核对每组数据的每次卷积都生成一个新的像素。卷积核沿像素层数据的x轴方向和y轴方向两个方向移动，移动的步长是1个像素。因此，运算后的卷积核的尺寸为(13-3+12)/1+1=13（13个像素减去3，正好是10，在加上上下、左右各填充的1个像素，即生成12个像素，再加上被减去的3也对应生成一个像素），每个GPU中共1313128个卷积核。2个GPU中共1313256个卷积后的像素层。这些像素层经过relu5单元的处理，生成激活像素层，尺寸仍为2组1313128像素层，共1313*256个像素层。

2组1313128像素层分别在2个不同GPU中进行池化(pool)运算处理。池化运算的尺度为33，运算的步长为2，则池化后图像的尺寸为(13-3)/2+1=6。 即池化后像素的规模为两组66128的像素层数据，共66*256规模的像素层数据。

### 3.6 全连接层1

第六层输出的4096个数据与第七层的4096个神经元进行全连接，然后经由relu7进行处理后生成4096个数据，再经过dropout7处理后输出4096个数据。 

### 3.7 全连接层2

第六层输出的4096个数据与第七层的4096个神经元进行全连接，然后经由relu7进行处理后生成4096个数据，再经过dropout8处理后输出4096个数据。 

### 3.8 全连接层3

第七层输出的4096个数据与第八层的1000个神经元进行全连接，经过训练后输出被训练的数值。 

## AlexNet为啥取得比较好的结果呢？

### 4.1 使用了ReLU函数

基于ReLU的深度卷积网络比基于tanh和sigmoid的网络训练快数倍。

### 4.2 标准化BN

使用ReLU f(x)=max(0,x)后，你会发现激活函数之后的值没有了tanh、sigmoid函数那样的一个阈值区间，所以一般在ReLU之后会做一个标准化。

### 4.3 Dropout

能够比较有效地防止神经网络的过拟合。 相对于一般如线性模型使用正则的方法来防止模型过拟合，而在神经网络中Dropout通过修改神经网络本身结构来实现。对于某一层神经元，通过定义的概率来随机删除一些神经元，同时保持输入层与输出层神经元的个人不变，然后按照神经网络的学习方法进行参数更新，下一次迭代中，重新随机删除一些神经元，直至训练结束。 

### 4.4 数据增强

深度学习中，当数据量不大的时候，一般有4种解决方法：

》》人工增加训练集的大小，通过平移，翻转，加噪声等方法从已有数据中创造。

》》正则化：数据量比较小会导致模型的过拟合，使得训练误差很小，而测试误差很大。通过Loss Funtion损失函数后面加上正则化项可以抑制过拟合的产生，缺点需要引入一个手动调整的hyper-parameter。

》》Dropout：也是一种正则化的手段。

》》Unsupervised Pre-training-------用Auto-Encoder或者RBM的卷积形式一层一层地做无监督训练，最后加上分类层做有监督的微调
